{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries needed\n",
    "import numpy as np\n",
    "import scipy.sparse.linalg\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA and LDA comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task for this exercise is to compare PCA and LDA in their ability to cluster when projecting very high-dimensional datapoints to 2 or 3 dimensions. \n",
    "\n",
    "In particular, consider the dataset MNIST provided on Virtuale. This dataset contains images of handwritten digits with dimension 28x28, together with a number from 0 to 9 representing the label. \n",
    "\n",
    "You are asked to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load the dataset in memory and explore its head and shape to understand how the informations are placed inside of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data into memory\n",
    "data = pd.read_csv('./data.csv')\n",
    "\n",
    "print(data.head())\n",
    "print(data.shape)\n",
    "\n",
    "# We can skip data cleaning because dataset is already processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Split the dataset into the X matrix of dimension dxN, with d = 784 (28 x 28) being the dimension of each datum, N (42000) is the number of datapoints, and Y $\\in$ $R^N$ containing the corresponding labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset has 42.000 observation of 28 x 28 dimesion\n",
    "\n",
    "Flattening: when you take a matrix and convert it into a vector by concatenating the rows. \n",
    "There is an additional coulumn (the first one) which is the label of the row. \n",
    "It represent the number hand written in the matrix flattered in the row.\n",
    "\n",
    "flattening 28 x 28 -> 784 = 28*28 dimensional vector\n",
    "\n",
    "We now have to split data into a matrix X and a vector Y where:\n",
    "- X is dimension (784, 42000)\n",
    "- Y is dimension (42000, )\n",
    "- Y is the first coulumn of data, while X is the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert data into a matrix\n",
    "data = np.array(data)   #we will lose the name of the columns and the row number\n",
    "print(data.shape)\n",
    "\n",
    "X0 = data[:, 1:].T\n",
    "print(X0.shape)\n",
    "\n",
    "Y0 = data[:, 0]\n",
    "print(Y0.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to visualize images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the image of index 'idx' from the dataset 'X'\n",
    "def visualize(X, idx):\n",
    "    #Load an image in memory\n",
    "    img = X[:,idx] \n",
    "\n",
    "    #Reshaspe --> opposite of flattening: from vector to matrix\n",
    "    img = np.reshape(img, (28, 28)) \n",
    "\n",
    "    #Visualize\n",
    "    plt.imshow(img, cmap='gray')     #cmap: how to convert pixel in colors\n",
    "    plt.show()\n",
    "\n",
    "#Visualize image number 9 and the corresponding digit\n",
    "idx = [index for index, elem in enumerate(Y0) if elem == 9]\n",
    "visualize(X0, idx[0])\n",
    "print(f\"The associated digit is: {Y0[idx[0]]}\") #the correspondence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Choose a number of digits (for example, 0, 6 and 9) and extract from X and Y the sub-dataset containing only the considered digits. \n",
    "Re-call X and Y those datasets, since the originals are not required anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [0, 6, 9]\n",
    "idx = [index for index, elem in enumerate(Y0) if elem in classes]\n",
    "\n",
    "X = X0[:, idx]     #take all the rows and just the columns idx\n",
    "Y = Y0[idx]\n",
    "\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Set N_train < N and randomly sample a training set with N_train datapoints from X (and the corresponding Y). Call them X_train and Y_train. Everything else is the test set. Call it X_test and Y_test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function implementing the split. It takes as input the dataset X and an integer Ntrain and returns 4 arrays \n",
    "- Xtrain: composed by Ntrain random samples from X\n",
    "- Xtest: is the rest of the array X\n",
    "- Ytrain and Ytest accordingly\n",
    "\n",
    "Limitation: the sample has to be random (numpy.random.shuffle)\n",
    "Example: if Ntrain = 30000 on our MNIST data, the shapes should be:\n",
    "- Xtrain  (784, 30000)\n",
    "- Xtest (784, 12000)\n",
    "- Ytrain (30000, )\n",
    "- Ytest (12000, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, Y, Ntrain):\n",
    "\n",
    "    d, N = X.shape\n",
    "\n",
    "    idx = np.arange(N)\n",
    "    np.random.shuffle(idx)\n",
    "\n",
    "    train_idx = idx[:Ntrain]\n",
    "    test_idx = idx[Ntrain:]\n",
    "\n",
    "    Xtrain = X[:, train_idx]\n",
    "    Ytrain = Y[train_idx]\n",
    "    \n",
    "    Xtest = X[:, test_idx]\n",
    "    Ytest = Y[test_idx]\n",
    "\n",
    "    return (Xtrain, Ytrain), (Xtest, Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ntrain = 8000\n",
    "\n",
    "(Xtrain, Ytrain) ,(Xtest, Ytest) = split_data(X,Y,Ntrain)\n",
    "print(Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Implement the algorithms computing the PCA and LDA of Xtrain with a fixed k. Visualize the results (for k = 2) and the position of the centroid of each cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement PCA, we first need to center the data. This can be done by defining its centroid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centroid(X):\n",
    "    return np.mean(X, axis=1)   #axis = 1: applied by row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA(X, k):\n",
    "    centroidX = get_centroid(X)\n",
    "    r, = centroidX.shape\n",
    "    centroidX = np.reshape(centroidX, (r, 1))\n",
    "    X_c = X - centroidX\n",
    "\n",
    "    U, S, VT = np.linalg.svd(X_c, full_matrices=False) #full_matrices=false -> the shapes are (..., M, K) and (..., K, N), respectively, where K = min(M, N)\n",
    "\n",
    "    U_k = U[:, :k]\n",
    "    Z_k = U_k.T @ X_c\n",
    "\n",
    "    return Z_k, U_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDA(Xtrain, Ytrain, k):\n",
    "    d, N = Xtrain.shape\n",
    "\n",
    "    #Find the corresponding indeces\n",
    "    I1 = (Ytrain==0) \n",
    "    I2 = (Ytrain==6)\n",
    "    I3 = (Ytrain==9)\n",
    "\n",
    "    X1 = Xtrain[:, I1]\n",
    "    Y1 = Ytrain[I1]\n",
    "\n",
    "    X2 = Xtrain[:, I2]\n",
    "    Y2 = Ytrain[I2]\n",
    "\n",
    "    X3 = Xtrain[:, I3]\n",
    "    Y3 = Ytrain[I3]\n",
    "\n",
    "    X = np.concatenate((X1, X2, X3), axis=1)\n",
    "    Y = np.concatenate((Y1, Y2, Y3))\n",
    "\n",
    "    # Class centroids\n",
    "    c1 = get_centroid(X1)\n",
    "    c1 = np.reshape(c1, (d, 1))\n",
    "\n",
    "    c2 = get_centroid(X2)\n",
    "    c2 = np.reshape(c2, (d, 1))\n",
    "\n",
    "    c3 = get_centroid(X3)\n",
    "    c3 = np.reshape(c3, (d, 1))\n",
    "\n",
    "    # Global centroid\n",
    "    C = get_centroid(X)\n",
    "\n",
    "    #Within-clusters Scatter Matrix\n",
    "    X_c1 = X1 - c1  #c1 was reshaped to do difference between matrix and column vector \n",
    "    X_c2 = X2 - c2\n",
    "    X_c3 = X3 - c3\n",
    "\n",
    "    X_w = np.concatenate((X_c1, X_c2, X_c3), axis=1)\n",
    "\n",
    "    S_w = X_w @ X_w.T\n",
    "\n",
    "    # Between-clusters Scatter Matrix\n",
    "    X1_bar = np.repeat(c1, X1.shape[1], axis=1)\n",
    "    X2_bar = np.repeat(c2, X2.shape[1], axis=1)\n",
    "    X3_bar = np.repeat(c2, X3.shape[1], axis=1)\n",
    "\n",
    "    X_bar = np.concatenate((X1_bar, X2_bar, X3_bar), axis=1)\n",
    "   \n",
    "    Xc_bar = X_bar - C.reshape(d,1)\n",
    "    S_b = Xc_bar @ Xc_bar.T\n",
    "\n",
    "    #We want to compute the Cholesky decomposition\n",
    "    try:\n",
    "        L = np.linalg.cholesky(S_w)\n",
    "    except:\n",
    "        eps = 1e-6\n",
    "        S_w = S_w + eps * np.eye(S_w.shape[0])\n",
    "        \n",
    "        L = np.linalg.cholesky(S_w)\n",
    "\n",
    "    _, W = scipy.sparse.linalg.eigs(np.linalg.inv(L) @ S_b @ L, k=k) #the two largest eigenvalues andd the corresponding eigenvector\n",
    "    W = np.real(W)\n",
    "    \n",
    "    Q = np.linalg.inv(L).T @ W\n",
    "\n",
    "    Z = Q.T @ X\n",
    "\n",
    "    return Z, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(Z, Y, extraPoint = []):\n",
    "    zero=plt.scatter(Z[0,Y==0],Z[1,Y==0],marker=\"o\", color='green')\n",
    "    six=plt.scatter(Z[0,Y==6],Z[1,Y==6],marker=\"o\", color='blue')\n",
    "    nine=plt.scatter(Z[0,Y==9],Z[1,Y==9],marker=\"o\", color='pink')\n",
    "\n",
    "    clusters = [zero, six, nine]\n",
    "\n",
    "    c0 = get_centroid(Z[:, Y==0])\n",
    "    plt.scatter(c0[0], c0[1], marker='*', color='red')\n",
    "\n",
    "    c6 = get_centroid(Z[:, Y==6])\n",
    "    plt.scatter(c6[0], c6[1], marker='*', color='red')\n",
    "\n",
    "    c9 = get_centroid(Z[:, Y==9])\n",
    "    plt.scatter(c9[0], c9[1], marker='*', color='red')\n",
    "\n",
    "    centroids = [c0, c6, c9]\n",
    "\n",
    "    plt.grid()\n",
    "    plt.legend((zero,six,nine),(\"zero\",\"six\",\"nine\"))\n",
    "\n",
    "    if(len(extraPoint)!=0):\n",
    "        plt.scatter(extraPoint[0], extraPoint[1], marker='*', color=\"yellow\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return clusters, centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centroid_of_cluster(Z, Y, label):\n",
    "    '''\n",
    "    Z is the reduced dataset\n",
    "    Y is the labels array\n",
    "    label is the class of the cluster\n",
    "    '''\n",
    "    return get_centroid(Z[:, Y==label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA\n",
    "print(\"PCA\")\n",
    "Z_pca, proj_PCA = PCA(Xtrain, 2)\n",
    "print(Z_pca.shape)\n",
    "cluster_pca, centroid_pca = plot(Z_pca, Ytrain)\n",
    "\n",
    "#LDA\n",
    "print(\"LDA\")\n",
    "Z_lda, proj_LDA = LDA(Xtrain, Ytrain, 2)\n",
    "Y1 = Ytrain[Ytrain==0]\n",
    "Y2 = Ytrain[Ytrain==6]\n",
    "Y3 = Ytrain[Ytrain==9]\n",
    "Y_lda = np.concatenate((Y1,Y2,Y3))\n",
    "cluster_lda, centroid_lda = plot(Z_lda, Y_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. For both the algorithms, compute for each cluster the average distance from the centroid. Comment the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_cluster_centroid(clusters, centroids):\n",
    "    centroid_index = 0\n",
    "    distances = []\n",
    "\n",
    "    for el in clusters:\n",
    "        points = el.get_offsets().data\n",
    "        r, c = points.shape\n",
    "\n",
    "        dist_point_centr = []\n",
    "\n",
    "        for i in range(r):\n",
    "            d = np.linalg.norm(points[i] - centroids[centroid_index], 2)\n",
    "            dist_point_centr.append(d)\n",
    "\n",
    "        average = sum(dist_point_centr)/r\n",
    "        distances.append(average)\n",
    "\n",
    "        centroid_index += 1\n",
    "    \n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_pca = distance_cluster_centroid(cluster_pca, centroid_pca)\n",
    "print(f\"PCA distances: 0 --> {distances_pca[0]}, \\t 6--> {distances_pca[1]}, \\t 9--> {distances_pca[2]}\")\n",
    "\n",
    "distances_lda = distance_cluster_centroid(cluster_lda, centroid_lda)\n",
    "print(f\"LDA distances: 0 --> {distances_lda[0]}, \\t 6--> {distances_lda[1]}, \\t 9--> {distances_lda[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. For both the algorithms, compute for each cluster the average distance from the centroid on the test set. Comment the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroidX_test = get_centroid(Xtest)\n",
    "Z_pca_test =  proj_PCA.T @ (Xtest - np.reshape(centroidX_test, (len(centroidX_test), 1)))\n",
    "clusters_pca, centroids_pca = plot(Z_pca_test, Ytest)\n",
    "\n",
    "Z_lda_test = proj_LDA.T @ Xtest\n",
    "clusters_lda, centroids_lda = plot(Z_lda_test, Ytest)\n",
    "\n",
    "distances_pca_test = distance_cluster_centroid(clusters_pca, centroids_pca)\n",
    "print(f\"PCA distances: 0 --> {distances_pca_test[0]}, \\t 6--> {distances_pca_test[1]}, \\t 9--> {distances_pca_test[2]}\")\n",
    "\n",
    "distances_lda_test = distance_cluster_centroid(clusters_lda, centroids_lda)\n",
    "print(f\"LDA distances: 0 --> {distances_lda_test[0]}, \\t 6--> {distances_lda_test[1]}, \\t 9--> {distances_lda_test[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Define a classification algorithm in this way: given a new observation x, compute the distance between x and each cluster centroid. Assign x to the class corresponding the closer centroid. Compute the accuracy of this algorithm on the test set and compute its accuracy for both PCA and LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check --> new observation ? xtest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def classification_algorithm(obs, centroids, gt):\n",
    "    distance_proj_clusters = []\n",
    "\n",
    "    for i in range (len(centroids)):\n",
    "        d = np.linalg.norm(obs - centroids[i], 2)\n",
    "        distance_proj_clusters.append(d)\n",
    "\n",
    "    min_index = np.argmin(distance_proj_clusters)\n",
    "\n",
    "    predicted_class = classes[min_index]\n",
    "\n",
    "    return predicted_class == gt\n",
    "\n",
    "\n",
    "def accuracy(X, centroids, Y):\n",
    "    results = [classification_algorithm(X[:, i], centroids, gt) for i, gt in enumerate(Y)]\n",
    "\n",
    "    correct_pred = np.sum(results)\n",
    "\n",
    "    correctness = correct_pred / len(Y)\n",
    "    print(f'Percentage of correct classifications: {correctness}')\n",
    "    \n",
    "    return correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('PCA')\n",
    "accuracy_pca = accuracy(Z_pca_test, centroids_pca, Ytest)\n",
    "\n",
    "print('LDA')\n",
    "accuracy_lda = accuracy(Z_lda_test, centroids_lda, Ytest) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Repeat this experiment for different values of k and different digits. What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA troppo basso ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Zpca, new_proj_PCA = PCA(Xtrain, 10)\n",
    "Zlda, new_proj_LDA = LDA(Xtrain, Ytrain, 10)\n",
    "\n",
    "centroidX_test = get_centroid(Xtest)\n",
    "Xtest_pca_new = new_proj_PCA.T @ (Xtest - np.reshape(centroidX_test, (len(centroidX_test), 1)))\n",
    "\n",
    "Xtest_lda_new = new_proj_LDA.T @ Xtest\n",
    "\n",
    "centroids_pca_new = []\n",
    "for label in classes:\n",
    "    centroids_pca_new.append(centroid_of_cluster(Zpca, Ytrain, label))\n",
    "\n",
    "centroids_lda_new = []\n",
    "for label in classes:\n",
    "    centroids_lda_new.append(centroid_of_cluster(Zlda, Ytrain, label))\n",
    "\n",
    "print('PCA')\n",
    "accuracy_pca = accuracy(Xtest_pca_new, centroids_pca_new, Ytest)\n",
    "\n",
    "print('LDA')\n",
    "accuracy_lda = accuracy(Xtest_lda_new, centroids_lda_new, Ytest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
