{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries needed\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization via Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, we want to study methods to solve the general optimization problem where,\n",
    "given a function $f : \\mathbb{R}^n → \\mathbb{R}$, we want to compute\n",
    "\n",
    "$$ x^* = \\underset{x ∈\\mathbb{R}^n}{arg\\,min}\\;f(x) $$ \n",
    "\n",
    "In particular, we will consider the situation where $f(x)$ is at least differentiable, which implies that we can compute its gradient $∇f(x)$.\n",
    "\n",
    "In this framework, one of the most common way to approach is to use the Gradient Descent (GD)\n",
    "method, which is an iterative algorithm that, given an initial iterate $x_0 ∈\\mathbb{R}^n$ and a positive parameter called\n",
    "step size $α_k > 0$ for each iteration, computes\n",
    "\n",
    "$$ x_{k+1} = x_k − α_k∇f(x_k) $$\n",
    "\n",
    "You are asked to implement the GD method in Python and to test it with some remarkable functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write a script that implement the GD algorithm, with the following structure:\n",
    "\n",
    "        Input:\n",
    "\n",
    "                f: the function f(x) we want to optimize.\n",
    "\n",
    "                It is supposed to be a Python function, not an array.\n",
    "\n",
    "                grad_f: the gradient of f(x). It is supposed to be a Python function, not an array.\n",
    "\n",
    "                x0: an n-dimensional array which represents the initial iterate.\n",
    "\n",
    "                kmax: an integer. The maximum possible number of iterations (to avoid infinite loops)\n",
    "\n",
    "                tolf: small float. The relative tollerance of the algorithm.\n",
    "\n",
    "                Convergence happens if ||grad_f(x_k)||_2 < tolf ||grad_f(x_0)||_2\n",
    "                tolx: small float. The tollerance in the input domain.\n",
    "                Convergence happens if ||x_{k} - x_{k-1}||_2 < tolx.\n",
    "\n",
    "        Pay attention to to the first iterate.\n",
    "        \n",
    "\n",
    "        Output:\n",
    "\n",
    "                x: an array that contains the value of x_k FOR EACH iterate x_k (not only the latter).\n",
    "\n",
    "                k: an integer. The number of iteration needed to converge. k < kmax.\n",
    "\n",
    "                f_val: an array that contains the value of f(x_k) FOR EACH iterate x_k.\n",
    "\n",
    "                grads: an array that contains the value of grad_f(x_k) FOR EACH iterate x_k.\n",
    "\n",
    "                err: an array the contains the value of ||grad_f(x_k)||_2 FOR EACH iterate x_k.\n",
    "\n",
    "        For the moment, consider a fixed value of α > 0.\n",
    "\n",
    "<br>\n",
    "\n",
    "- It's given the implementation of the backtracking algorithm for the GD method. That function works as follows:\n",
    "\n",
    "        Input:\n",
    "        \n",
    "                f: the function f(x) we want to optimize.\n",
    "\n",
    "                It is supposed to be a Python function, not an array.\n",
    "\n",
    "                grad_f: the gradient of f(x). It is supposed to be a Python function, not an array.\n",
    "\n",
    "                x: an array. The actual iterate x_k for which you want to find the correct value for alpha.\n",
    "\n",
    "        Output:\n",
    "\n",
    "                alpha: a float. The correct step size for the next iteration.\n",
    "\n",
    "        Modify the code for the GD method to let it be able to use the backtracking algorithm for the choice of the step size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x**2 + 2*x +1\n",
    "\n",
    "def grad_f(x):\n",
    "    return (2*x + 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtracking(f, grad_f, x):\n",
    "    \"\"\"\n",
    "    This function is a simple implementation of the backtracking algorithm for\n",
    "    the GD (Gradient Descent) method.\n",
    "    \n",
    "    f: function. The function that we want to optimize.\n",
    "    grad_f: function. The gradient of f(x).\n",
    "    x: ndarray. The actual iterate x_k.\n",
    "    \"\"\"\n",
    "    alpha = 1\n",
    "    c = 0.8\n",
    "    tau = 0.25\n",
    "    \n",
    "    while f(x - alpha * grad_f(x)) > f(x) - c * alpha * np.linalg.norm(grad_f(x)) ** 2:\n",
    "        alpha = tau * alpha\n",
    "        \n",
    "        if alpha < 1e-3:\n",
    "            break\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, grad_f, x0, kmax, tolf, tolx, step=1, useBackTracking=True):\n",
    "    \n",
    "    # Initialization\n",
    "    k = 0\n",
    "\n",
    "    x = np.empty((kmax+1,))\n",
    "    f_val = np.empty((kmax+1, ))\n",
    "    grads = np.empty((kmax+1, ))\n",
    "    err_val = np.empty((kmax+1, ))\n",
    "    \n",
    "    # Assign the values for the first iteration\n",
    "    x[k]=x0\n",
    "    f_val[k] = f(x0)\n",
    "    grads[k] = grad_f(x0)\n",
    "    err_val[k] = np.linalg.norm(grad_f(x0))\n",
    "    \n",
    "    # Choose step size\n",
    "    alpha = step\n",
    "    \n",
    "    # Handle the condition for the first iteration\n",
    "    k+=1\n",
    "    x[k]=x[k-1]-alpha*grad_f(x[k-1])\n",
    "    f_val[k] = f(x[k])\n",
    "    grads[k] = grad_f(x[k])\n",
    "    err_val[k] = np.linalg.norm(grad_f(x[k]))\n",
    "\n",
    "    #Conditions\n",
    "    cond1 = np.linalg.norm(grad_f(x)) > tolf * grad_f(x0)\n",
    "    cond2 = np.linalg.norm(x - x0) > tolx * np.linalg.norm(x0)\n",
    "\n",
    "    # Start the iterations\n",
    "    while cond1 and cond2 and k<kmax:\n",
    "\n",
    "        #Update k\n",
    "        k = k+1\n",
    "        \n",
    "        # Update the value of x\n",
    "        x[k] = x[k-1]-alpha*grad_f(x[k-1])\n",
    "        \n",
    "        #Update alpha\n",
    "        if useBackTracking:\n",
    "            alpha = backtracking(f, grad_f, x[k])\n",
    "        \n",
    "        #Update values \n",
    "        f_val[k] = f(x[k])\n",
    "        grads[k] = grad_f(x[k])\n",
    "        err_val[k] = np.linalg.norm(grad_f(x[k]))\n",
    "    \n",
    "    #Truncate the vectors that are (eventually) too long\n",
    "    x = x[:k+1]\n",
    "    f_val = f_val[:k+1]\n",
    "    grads = grads[:k+1]\n",
    "    err_val = err_val[:k+1]\n",
    "    \n",
    "    return x, k, f_val, grads, err_val"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
