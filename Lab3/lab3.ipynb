{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries needed\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization via Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, we want to study methods to solve the general optimization problem where,\n",
    "given a function $f : \\mathbb{R}^n → \\mathbb{R}$, we want to compute\n",
    "\n",
    "$$ x^* = \\underset{x ∈\\mathbb{R}^n}{arg\\,min}\\;f(x) $$ \n",
    "\n",
    "In particular, we will consider the situation where $f(x)$ is at least differentiable, which implies that we can compute its gradient $∇f(x)$.\n",
    "\n",
    "In this framework, one of the most common way to approach is to use the Gradient Descent (GD)\n",
    "method, which is an iterative algorithm that, given an initial iterate $x_0 ∈\\mathbb{R}^n$ and a positive parameter called\n",
    "step size $α_k > 0$ for each iteration, computes\n",
    "\n",
    "$$ x_{k+1} = x_k − α_k∇f(x_k) $$\n",
    "\n",
    "You are asked to implement the GD method in Python and to test it with some remarkable functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write a script that implement the GD algorithm, with the following structure:\n",
    "\n",
    "        Input:\n",
    "\n",
    "                f: the function f(x) we want to optimize.\n",
    "\n",
    "                It is supposed to be a Python function, not an array.\n",
    "\n",
    "                grad_f: the gradient of f(x). It is supposed to be a Python function, not an array.\n",
    "\n",
    "                x0: an n-dimensional array which represents the initial iterate.\n",
    "\n",
    "                kmax: an integer. The maximum possible number of iterations (to avoid infinite loops)\n",
    "\n",
    "                tolf: small float. The relative tollerance of the algorithm.\n",
    "\n",
    "                Convergence happens if ||grad_f(x_k)||_2 < tolf ||grad_f(x_0)||_2\n",
    "                tolx: small float. The tollerance in the input domain.\n",
    "                Convergence happens if ||x_{k} - x_{k-1}||_2 < tolx.\n",
    "\n",
    "        Pay attention to to the first iterate.\n",
    "        \n",
    "\n",
    "        Output:\n",
    "\n",
    "                x: an array that contains the value of x_k FOR EACH iterate x_k (not only the latter).\n",
    "\n",
    "                k: an integer. The number of iteration needed to converge. k < kmax.\n",
    "\n",
    "                f_val: an array that contains the value of f(x_k) FOR EACH iterate x_k.\n",
    "\n",
    "                grads: an array that contains the value of grad_f(x_k) FOR EACH iterate x_k.\n",
    "\n",
    "                err: an array the contains the value of ||grad_f(x_k)||_2 FOR EACH iterate x_k.\n",
    "\n",
    "        For the moment, consider a fixed value of α > 0.\n",
    "\n",
    "<br>\n",
    "\n",
    "- It's given the implementation of the backtracking algorithm for the GD method. That function works as follows:\n",
    "\n",
    "        Input:\n",
    "        \n",
    "                f: the function f(x) we want to optimize.\n",
    "\n",
    "                It is supposed to be a Python function, not an array.\n",
    "\n",
    "                grad_f: the gradient of f(x). It is supposed to be a Python function, not an array.\n",
    "\n",
    "                x: an array. The actual iterate x_k for which you want to find the correct value for alpha.\n",
    "\n",
    "        Output:\n",
    "\n",
    "                alpha: a float. The correct step size for the next iteration.\n",
    "\n",
    "        Modify the code for the GD method to let it be able to use the backtracking algorithm for the choice of the step size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x**2 + 2*x +1\n",
    "\n",
    "def grad_f(x):\n",
    "    return (2*x + 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtracking(f, grad_f, x):\n",
    "    \"\"\"\n",
    "    This function is a simple implementation of the backtracking algorithm for\n",
    "    the GD (Gradient Descent) method.\n",
    "    \n",
    "    f: function. The function that we want to optimize.\n",
    "    grad_f: function. The gradient of f(x).\n",
    "    x: ndarray. The actual iterate x_k.\n",
    "    \"\"\"\n",
    "    alpha = 1\n",
    "    c = 0.8\n",
    "    tau = 0.25\n",
    "    \n",
    "    while f(x - alpha * grad_f(x)) > f(x) - c * alpha * np.linalg.norm(grad_f(x)) ** 2:\n",
    "        alpha = tau * alpha\n",
    "        \n",
    "        if alpha < 1e-3:\n",
    "            break\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GD(f, grad_f, x0, kmax, tolf, tolx, step=0.001, useBackTracking=True):\n",
    "    \n",
    "    # Initialization\n",
    "    k = 0\n",
    "\n",
    "    dim_m, dim_n = (kmax+1, ) + x0.shape\n",
    "\n",
    "    x = np.empty((dim_m, dim_n))\n",
    "    f_val = np.empty((kmax+1, ))\n",
    "    grads = np.empty((dim_m, dim_n))\n",
    "    err_val = np.empty((kmax+1, ))\n",
    "    \n",
    "    # Assign the values for the first iteration\n",
    "    x[k]=x0\n",
    "    f_val[k] = f(x0)\n",
    "    grads[k] = grad_f(x0)\n",
    "    err_val[k] = np.linalg.norm(grad_f(x0))\n",
    "    \n",
    "    # Choose step size\n",
    "    alpha = step\n",
    "    \n",
    "    # Handle the condition for the first iteration\n",
    "    k+=1\n",
    "    x[k]=x[k-1]-alpha*grad_f(x[k-1])\n",
    "    f_val[k] = f(x[k])\n",
    "    grads[k] = grad_f(x[k])\n",
    "    err_val[k] = np.linalg.norm(grad_f(x[k]))\n",
    "\n",
    "    #Conditions\n",
    "    cond1 = (np.linalg.norm(grad_f(x[k])) > tolf * grad_f(x[k-1])).all()\n",
    "    cond2 = (np.linalg.norm(x[k] - x[k-1]) > tolx * np.linalg.norm(x[k-1])).all()\n",
    "\n",
    "    conditions = cond1 and cond2\n",
    "\n",
    "    # Start the iterations\n",
    "    while (conditions and k < kmax):\n",
    "\n",
    "        #Update k\n",
    "        k = k+1\n",
    "        \n",
    "        # Update the value of x\n",
    "        x[k] = x[k-1]-alpha*grad_f(x[k-1])\n",
    "        \n",
    "        #Update alpha\n",
    "        if useBackTracking:\n",
    "            alpha = backtracking(f, grad_f, x[k])\n",
    "        \n",
    "        #Update values \n",
    "        f_val[k] = f(x[k])\n",
    "        grads[k] = grad_f(x[k])\n",
    "        err_val[k] = np.linalg.norm(grad_f(x[k]))\n",
    "\n",
    "        #Update condintions\n",
    "        cond1 = np.linalg.norm(grad_f(x[k])) > tolf * grad_f(x[k-1]).all()\n",
    "        cond2 = np.linalg.norm(x[k] - x[k-1]) > tolx * np.linalg.norm(x[k-1]).all()\n",
    "        conditions = cond1 and cond2\n",
    "    \n",
    "    #Truncate the vectors that are (eventually) too long\n",
    "    x = x[:k+1]\n",
    "    f_val = f_val[:k+1]\n",
    "    grads = grads[:k+1]\n",
    "    err_val = err_val[:k+1]\n",
    "    \n",
    "    return x, k, f_val, grads, err_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.9999938]\n"
     ]
    }
   ],
   "source": [
    "x, k, fval, grads, errval = GD(f, grad_f, np.asarray([2]), 1000, 1e-6, 1e-6)\n",
    "print(x[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test the algorithm above on the following functions:\n",
    "1. $f : \\mathbb{R}^2 → \\mathbb{R}$ such that\n",
    "\n",
    "    $f(x_1, x_2) = (x_1 − 3)^2 + (x_2 − 1)^2$\n",
    "    \n",
    "    for which the true optimum is $x^* = (3, 1)^T$\n",
    "\n",
    "<br>\n",
    "\n",
    "2. $f : \\mathbb{R}^2 → \\mathbb{R}$ such that\n",
    "\n",
    "    $f(x1, x2) = 10(x_1 − 1)^2 + (x_2 − 2)^2$\n",
    "\n",
    "    for which the true optimum is $x^* = (1, 2)^T$\n",
    "\n",
    "<br>\n",
    "\n",
    "3. $f : \\mathbb{R}^n → \\mathbb{R}$ such that\n",
    "\n",
    "    $f(x) = \\frac{1}{2}||Ax − b||_2^2$\n",
    "\n",
    "    where $A ∈ \\mathbb{R}^{n×n}$ is the Vandermonde matrix associated with the vector $v ∈ \\mathbb{R}^n$ that contains $n$ equispaced values in the interval $[0, 1]$, and $b ∈ \\mathbb{R}^n$ is computed by first setting $x_{true} = (1, 1, . . . , 1)^T$ and then $b = Ax_{true}$. Try for different values of $n$ (e.g. $n = 5, 10, 15, . . .$)\n",
    "\n",
    "<br>\n",
    "\n",
    "4. $f : \\mathbb{R}^n → \\mathbb{R}$ such that\n",
    "\n",
    "    $f(x) = \\frac{1}{2}||Ax − b||_2^2+\\frac{λ}{2}||x||_2^2$\n",
    "    \n",
    "    where $A$ and $b$ are the same of the exercise above, while $λ$ is a fixed value in the interval $[0, 1]$.\n",
    "    Try different values for $λ$.\n",
    "\n",
    "<br>\n",
    "\n",
    "5. $f : \\mathbb{R} -> \\mathbb{R}$ suche that\n",
    "\n",
    "    $f(x) = x^4 + x^3 -2x^2 - 2x$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For each of the functions above, run the GD method with and without the backtracking, trying different values for the step size $\\alpha > 0$ when you are not using backtracking. Observe the different behavior of GD.\n",
    "<br>\n",
    "- To help visualization, it is convenient to plot the error vector that contains the $||∇f(x_k)||_2$, to check that it goes to zero. Compare the convergence speed (in terms of the number of iterations k) in the different cases.\n",
    "<br>\n",
    "- For each of the points above, fix $x_0= (0, 0, ..., 0)^T$, kmax = 100, while choose your values for tolf and tolx. It is recommended to also plot the error $||x_k - x^*||_2$ varying k when the true $x^*$ is available.\n",
    "- Only for the non-convex function defined in 5, plot it in the interval [-3; 3] and test the convergence point of GD with different values of $x_0$ and different step-sizes. Observe when the convergence point is the global minimum and when it stops on a local minimum or maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = sys.float_info.epsilon\n",
    "\n",
    "def f1(x):\n",
    "    return (x[0]-3)**2 + (x[1]-1)**2\n",
    "\n",
    "def grad_f1(x):\n",
    "    grad = np.empty(len(x))\n",
    "    grad[0] = 2*(x[0]-3)\n",
    "    grad[1] = 2*(x[1]-1)\n",
    "\n",
    "    return grad\n",
    "\n",
    "\n",
    "def f2(x):\n",
    "    return 10*(x[0]-1)**2 + (x[1]-2)**2    \n",
    "\n",
    "def grad_f2(x):\n",
    "    grad = np.empty(len(x))\n",
    "    grad[0] = 20*(x[0]-1)\n",
    "    grad[1] = 2*(x[1]-2)\n",
    "\n",
    "    return grad\n",
    "\n",
    "\n",
    "def f3(x, n):\n",
    "    v = np.arange(0, 1+eps, 1/(n-1))\n",
    "    A = np.vander(v)\n",
    "    x_true = np.ones((n, ))\n",
    "    b = A@x_true\n",
    "\n",
    "    y = 1/2 * (np.linalg.norm(A@x - b))**2\n",
    "\n",
    "    return y\n",
    "\n",
    "def grad_f3(x, n):\n",
    "    v = np.arange(0, 1+eps, 1/(n-1))\n",
    "    A = np.vander(v)\n",
    "    x_true = np.ones((n, ))\n",
    "    b = A@x_true\n",
    "\n",
    "    grad = A.T @ (A@x - b)\n",
    "    \n",
    "    return grad\n",
    "\n",
    "def f4(x, n, L):\n",
    "    v = np.arange(0, 1+eps, 1/(n-1))\n",
    "    A = np.vander(v)\n",
    "    x_true = np.ones((n, ))\n",
    "    b = A@x_true\n",
    "\n",
    "    y1 = 1/2 * (np.linalg.norm(A@x - b))**2\n",
    "    y2 = L/2 * (np.linalg.norm(x))**2\n",
    "\n",
    "    return y1 + y2\n",
    "\n",
    "def grad_f4(x, n, L):\n",
    "    v = np.arange(0, 1+eps, 1/(n-1))\n",
    "    A = np.vander(v)\n",
    "    x_true = np.ones((n, ))\n",
    "    b = A@x_true\n",
    "\n",
    "    grad = A.T @ (A@x - b) + L*x\n",
    "\n",
    "    return grad\n",
    "\n",
    "def f5(x):\n",
    "    return x**4 + x**3 - 2*x**2 - 2*x\n",
    "\n",
    "def grad_f5(x):\n",
    "    return 4*x**3 + 3*x**2 - 4*x -2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Test funciton 1\n",
    "\n",
    "$f(x_1, x_2) = (x_1 − 3)^2 + (x_2 − 1)^2$\n",
    "\n",
    "We are going to test the the GD methon on the function with and without backtracking.\n",
    "When we are not using backtracking we are testing with learning rate of $1$, $0.5$, $0.1$, $0.03$ and $0.01$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.asarray([0,0])\n",
    "tolx = 1e-6\n",
    "tolf = 1e-6\n",
    "kmax = 10000\n",
    "\n",
    "cvector = np.asarray([1, 0.5, 0.1, 0.03, 0.01])\n",
    "\n",
    "f_tot = []\n",
    "x_tot = []\n",
    "\n",
    "for c in cvector:\n",
    "    x, k, f_val, grads, err_val  = GD(f1, grad_f1, x0, kmax, tolf, tolx, step=c, useBackTracking=False)\n",
    "    plt.plot()\n",
    "    \n",
    "    f_tot.append(f_val)\n",
    "    x_tot.append(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
